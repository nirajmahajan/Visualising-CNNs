# Visualising-CNNs

PyTorch Implementation of the paper [Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150)

Implemented as a course project for CS 689 (Machine Learning: Theory and Methods) at IIT Bombay.

## Introduction

Visualisation techniques for CNNs are an important field to understand the "blackbox" our conventional CNNs are. This paper [1] introduces us to a widely used visualisation technique known as "Class Activation Mappings" (or CAMs). The authors propose a simple but effective technique to identify the object pixels in an image by extracting information from the conv layers output that is normally lost when passed through the FC layers. Despite the simplicity of the method, CAMs achieve comparable results with SOTA methods in object detection on popular datasets like ILSVRC.

In this project, I have explored the applications of CAM on several architecture-dataset combination and contrasted the results with another popular visualisation technique using saliency maps, proposed by *Simonyan et al* [2]. I have also implemented weakly supervised number detection in the wild as an application of feature localisation using Class Activation Mappings. Furthermore, to expose the vulnerabilities of CNNs to manipulations and attacks, I have demonstrated the change in the classification scores with variation in a few object and non object pixels.

## Setup

The code was trained using python3 with a requirement of the following libraries:

1. pytorch - 1.9.0
2. torchvision  - 0.2.1
3. numpy - 1.21.0
4. matplotlib - 3.3.4
5. PIL - 8.2.0
6. tqdm - 4.61.1

## Class Activation Mappings

*Zhou et al* [1] propose a pipeline of conv layers followed by a Global Average Pooling layer and a Single Linear Layer to generate classification scores (logits). Each of the channel generated by the conv layers produces a CAM matrix which highlights the locations used by the conv layers in the input image. Using the weights of the linear layer, we take the weighted combination of these CAM matrices to produce a single mapping which is displayed as our final class activation mapping. This mapping is upsampled to match the input image and overlayed for comparison.

![](https://github.com/nirajmahajan/Visualising-CNNs/blob/master/images/cam.jpg)

## Class Saliency Maps - Backprop

*Simonyan et al* [2] implement a more specific method to get the contribution of each pixel to the pre softmax classification score. To uderstand this, let us first assume a CNN without any activation functions, ie, a linear score generator. For any class c, the pre-Softmax score S_c is given by:

![](https://github.com/nirajmahajan/Visualising-CNNs/blob/master/images/eqn1.png)

But since the output generated by any CNN is highly non-linear, the above score can be approximated again with a linear function (by the first order Taylor expansion). 

![](https://github.com/nirajmahajan/Visualising-CNNs/blob/master/images/eqn2.png)

Note that in our experiments, and in almost all popular CNN architectures, the activation function being used is the ReLU operation. For any input image, the modes of the ReLUs (on/off) are known/fixed. The "on" relu nodes simply pass on the input value and so, at the end, there is no approximation in case of ReLUs and we get the exact weights using the above equation. 

Here, w is the derivative of the pre-Softmax score of class c, with respect to the input image pixel-values, given by:

![](https://github.com/nirajmahajan/Visualising-CNNs/blob/master/images/eqn3.png)

This derivative can easily be computed using the backprop operation implemented in the deep learning frameworks. 

## Datasets used

I have used several datasets for this project:

1. [CIFAR10 [3]](https://www.cs.toronto.edu/~kriz/cifar.html)
2. [CIFAR100 [3]](https://www.cs.toronto.edu/~kriz/cifar.html)
3. [MNIST [4]](http://yann.lecun.com/exdb/mnist/)
4. [Houses Dataset [5]](https://github.com/emanhamed/Houses-dataset) (Only the frontal images)
5. [SVT Dataset [6]](http://tc11.cvc.uab.es/datasets/SVT_1)

The directory structure should be as follows: 

```bash
src
└── dataset
    ├── CIFAR10
    │   ├── cifar-10-batches-py
    │   │   ├── batches.meta
    │   │   ├── data_batch_1
    │   │   ├── data_batch_2
    │   │   ├── data_batch_3
    │   │   ├── data_batch_4
    │   │   ├── data_batch_5
    │   │   ├── readme.html
    │   │   └── test_batch
    │   └── cifar-10-python.tar.gz
    ├── CIFAR100
    │   ├── cifar-100-python
    │   │   ├── meta
    │   │   ├── test
    │   │   └── train
    │   └── cifar-100-python.tar.gz
    ├── Digit_detection
    │   ├── 0
    │   │   ├── houses_image_0.png
    │   │   ├── houses_image_1.png
    │   │   ├── .....
    │   │   └── houses_image_n.png
    │   └── 1
    │       ├── svt_image_0.png
    │       ├── svt_image_1.png
    │       ├── .....
    │       └── svt_image_n.png
    └── MNIST
        ├── processed
        │   ├── test.pt
        │   └── training.pt
        └── raw
            ├── t10k-images-idx3-ubyte
            ├── t10k-labels-idx1-ubyte
            ├── train-images-idx3-ubyte
            └── train-labels-idx1-ubyte

```

NOTE: The CIFAR10, CIFAR100, MNIST datasets are downloaded from the loader provided by the pytorch library.

## Training Method

I have used the vgg16 architecture as the skeleton for all experiments. All the layers after the conv-5 are replaced by a Global Average Pooling and a Single Linear Layer.

![](https://github.com/nirajmahajan/Visualising-CNNs/blob/master/images/mod_vgg16.png)

## Usage of Code TODO

```bash
$ cd src/
$ 
```

The pre-trained weights are already loaded in the repository.

## Tuning of hyper parameters TODO

We have further tuned the hyper parameters for the training by running over 30 experiments. More details about the experiments can be found [here](https://github.com/nirajmahajan/Low-Light-Enhancement-Using-Deep-Retinex-Decomposition/tree/master/experiments).

## Results and Observations TODO 

In this section, we have attached our own results after training the model from scratch. The trained model gave good results in almost all cases. But we also noticed that in very few cases the Illuminance misbehaved, ie, gave extremely high values which generated abnormally bright and cloudy images.

All our generated results can be found [here](https://github.com/nirajmahajan/Low-Light-Enhancement-Using-Deep-Retinex-Decomposition/tree/master/results). We tested the performance on two separate sets of images:

1. Known Images: Test images that are from similar locations as the training data. (Of course they are not used for training)
2. Unknown Images: Completely foreign images

We have attached a few results here:

## Contributors

1. [Niraj Mahajan](https://www.cse.iitb.ac.in/~nirajm)

## References

1. L*earning Deep Features for Discriminative Localization*; 
   Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba.
2. *Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps*; 
   Karen Simonyan, Andrea Vedaldi, Andrew Zisserman
3. Learning Multiple Layers of Features from Tiny Images;
   Alex Krizhevsky, 2009.
4. MNIST handwritten digit database;
   LeCun, Yann and Cortes, Corinna
5. House price estimation from visual and textual features;
   Ahmed, Eman and Moustafa, Mohamed.
6. The Street View Text Dataset (SVT);
   Kai Wang, ID:SVT_1,URL:http://tc11.cvc.uab.es/datasets/SVT_1
